import importlib.util, pytest
if importlib.util.find_spec('django') is None:
    pytest.skip('django not installed; skipping module', allow_module_level=True)

import os, sys, types as _types, pytest as _pytest, warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=PendingDeprecationWarning)
_t = os.environ.get('TARGET_ROOT') or 'target'
if _t and os.path.isdir(_t):
    _p = os.path.abspath(os.path.join(_t, os.pardir))
    [sys.path.insert(0, p) for p in (_p,_t) if p not in sys.path]
    _pkg=_types.ModuleType('target'); _pkg.__path__=[_t]; sys.modules.setdefault('target', _pkg)

import pytest as _pytest
_pytest.skip('generator: banned private imports detected; skipping module', allow_module_level=True)

try:
    import pytest
    import types
    from conduit.apps.core.exceptions import core_exception_handler
    from conduit.apps.core.utils import generate_random_string
    from conduit.apps.authentication.models import User
    from conduit.apps.authentication.renderers import UserJSONRenderer
    from rest_framework import exceptions as drf_exceptions
except ImportError as e:
    import pytest as _pytest
    _pytest.skip(f"Required test dependencies not available: {e}", allow_module_level=True)

import json


@pytest.mark.parametrize(
    "exc, expected_status",
    [
        (drf_exceptions.NotFound("not here"), 404),
        (Exception("boom"), 500),
    ],
)
def test_core_exception_handler_handles_not_found_and_generic(exc, expected_status):
    # Arrange-Act-Assert: generated by ai-testgen
    # Arrange
    context = {"view": None}
    # Act
    response = core_exception_handler(exc, context)
    # Assert
    assert hasattr(response, "status_code")
    assert isinstance(response.status_code, int)
    assert response.status_code == expected_status
    # Response data should be a mapping containing error information
    assert hasattr(response, "data")
    assert isinstance(response.data, dict)
    # For NotFound we expect an 'errors' key; for generic errors at least an 'errors' or 'detail'
    assert "errors" in response.data or "detail" in response.data


def test_user_generate_jwt_and_get_short_name_and_renderer_output():
    # Arrange-Act-Assert: generated by ai-testgen
    # Arrange
    fake_user = types.SimpleNamespace(pk=99, username="alice")
    # Act
    token = User._generate_jwt_token(fake_user)
    short_name = User.get_short_name(fake_user)
    renderer = UserJSONRenderer()
    rendered = renderer.render({"user": {"username": "alice", "email": "a@b.c"}}, accepted_media_type=None, renderer_context=None)
    # Assert
    assert isinstance(token, str)
    # JWT tokens should contain two dots
    assert token.count(".") == 2
    assert isinstance(short_name, str)
    assert short_name == "alice"
    assert isinstance(rendered, (bytes, bytearray))
    # Ensure JSON structure contains the "user" key when decoded
    decoded = rendered.decode("utf-8")
    parsed = json.loads(decoded)
    assert "user" in parsed
    assert parsed["user"]["username"] == "alice"


@pytest.mark.parametrize(
    "length, repeat",
    [
        (8, 2),
        (16, 3),
    ],
)
def test_generate_random_string_length_and_uniqueness(length, repeat):
    # Arrange-Act-Assert: generated by ai-testgen
    # Arrange & Act
    results = [generate_random_string(length) for _ in range(repeat)]
    # Assert: correct length and type
    for s in results:
        assert isinstance(s, str)
        assert len(s) == length
    # Assert: uniqueness across generated strings in this small sample
    assert len(set(results)) == len(results)
